{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deep (L-Layer) Neural Network from scratch\n",
    "**We will :**\n",
    "- Build  the general architecture of an L layer Neural Network  learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order.\n",
    "<img src=./data/deep_neural_network.png><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "* [scikit-learn](http://scikit-learn.org/stable/) a library with Simple and efficient tools for data mining and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "we will use the make_classification data from sklearn\n",
    "\n",
    "Loading the data by with the  following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y=datasets.make_classification(n_samples=100000, n_features=100,\n",
    "                                    n_informative=100,n_classes=2, n_redundant=0,\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data-split  ##\n",
    "\n",
    "we will split the data with the following distribution \n",
    "- 99% -training set\n",
    "- 1% -test set\n",
    "\n",
    "we will use the sklearn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we reshape the data into  a numpy-array of shape (1, m). After this, our training (and test) dataset is a numpy-array where each column represents one training example. There should be m_train (respectively m_test) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to reshape our data to column vectors \n",
    "X_train=X_train.reshape(X_train.shape[0],-1).T\n",
    "X_test=X_test.reshape(X_test.shape[0],-1).T\n",
    "y_train=y_train.reshape(y_train.shape[0],-1).T\n",
    "y_test=y_test.reshape(y_test.shape[0],-1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features,number of layers) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will now initialize the number of layers with the specific units in each layer\n",
    "- We will store all these in a list called layer_dims\n",
    "- The first layer has n_x dimensions and the output layer has one unit for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_x,n_y=X_train.shape[0],y_train.shape[0]\n",
    "layer_dims=[n_x,30,20,40,n_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get the number of layers in the network from layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 We now initialize parameters for the network based on layer_dims values\n",
    "- We provide a variety of ways to initialize the parameters to see the effects of different initialization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_layers(layer_dims,initializer=\"random\"):\n",
    "    \"\"\"\n",
    "    This function initializes the parameters for different layers\n",
    "    Arguments:\n",
    "        layer_dims-> a list of layer dimensions \n",
    "        initializer -> type of initilization\n",
    "    \n",
    "    \"\"\"\n",
    "    #number of layers\n",
    "    L=len(layer_dims)\n",
    "    # variable for parameters\n",
    "    parameters={}\n",
    "    #create parameters\n",
    "    if initializer==\"random\":\n",
    "        for l in range(1,L):\n",
    "            parameters[\"W\"+ str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "            parameters[\"b\"+str(l)]=np.zeros((layer_dims[l],1),dtype=float)\n",
    "        return parameters\n",
    "    elif initializer==\"zeros\":\n",
    "        for l in range(L-1):\n",
    "            parameters[\"W\"+ str(l)]=np.zeros((layer_dims[l],layer_dims[l-1]),dtype=zeros)\n",
    "            parameters[\"b\"+str(l)]=np.zeros((layer_dims[l],1),dtype=float)\n",
    "        return parameters\n",
    "    elif initializer==\"xavier\":\n",
    "        for l in range(L-1):\n",
    "            parameters[\"W\"+ str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "            parameters[\"b\"+str(l)]=np.zeros((layer_dims[l],1),dtype=float)\n",
    "        return parameters\n",
    "    elif initializer==\"He\":\n",
    "        for l in range(L-1):\n",
    "            parameters[\"W\"+ str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "            parameters[\"b\"+str(l)]=np.zeros((layer_dims[l],1),dtype=float)\n",
    "        return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now define the various activations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <center>  Relu function </center> \n",
    "#    <center>               $\\max(0,Z)$</center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    This function computes the relu activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighteds inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->relu activations of Z\n",
    "    \"\"\"\n",
    "    A=np.maximum(0,Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  <center>  Tanh function </center> \n",
    "#    <center>               $\\frac{\\mathrm{e}^{z}-\\mathrm{e}^{-z}}{\\mathrm{e}^{z}+\\mathrm{e}^{-z}}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    This function computes the tanh activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighted inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->tanh activations of Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A=(np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <center>  Sigmoid function </center> \n",
    "#    <center>               $\\frac{1}{1+\\mathrm{e}^{-z}}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    This function computes the sigmoid activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighted inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->sigmoid activations of Z\n",
    "    \"\"\"\n",
    "    A= 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear forward computation\n",
    "- we use the following function to compute the linear function \n",
    "- We compute $A^{l} = \\sigma(W^{[l]^{T}} A^{[l-1]} + b^{[l]}) = (a^{(0)}, a^{(1)}, ..., a^{(n_l-1)}, a^{(n_l)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_computation(parameters,X, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Z= np.dot(paramet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation\n",
    "\n",
    "**forward propagation:** Implementing forward propagation \n",
    "\n",
    "** for layer hidden layers **\n",
    "- where l is layer number,L the total number of layers and n_l is the number of units in layer l\n",
    "- We get $A^{[l-1]}$ where $A^{0} = X$\n",
    "- We compute $A^{l} = \\sigma(W^{[l]^{T}} A^{[l-1]} + b^{[l]}) = (a^{(0)}, a^{(1)}, ..., a^{(n_l-1)}, a^{(n_l)})$\n",
    "- where  $\\sigma$ is the  activation function \n",
    "\n",
    "** for output layer **\n",
    "- We get layer [L-1] activations\n",
    "- we compute $A^{[L]}=\\sigma(W^{[L]^{T}} A^{[L-1]} + b^{[L]})=(a^{(0)}, a^{(1)}, ..., a^{(n_L-1)}, a^{(n_L)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_propagate_deep(parameters, X):\n",
    "    \"\"\"\n",
    "    This function computes the forward propagation for the network\n",
    "    \n",
    "    \"\"\"\n",
    "    L=len(parameters)//2 \n",
    "    cache={}\n",
    "    A=X\n",
    "    for i in range(1,L):\n",
    "        A_Prev=A\n",
    "        Z=np.dot(parameters[\"W\"+str(i)],A_Prev)+parameters[\"b\"+str(i)]\n",
    "        A=tanh(Z)\n",
    "        cache[\"Z\"+str(i)]=Z\n",
    "        cache[\"A\"+str(i)]=A\n",
    "    ZL=np.dot(parameters[\"W\"+str(L)],A)+parameters[\"b\"+str(L)]\n",
    "    AL=sigmoid(ZL)\n",
    "\n",
    "    return AL, cache\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "parameters=initialize_layers(layer_dims)\n",
    "al,cache=forward_propagate_deep(parameters,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5001167413983475"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
