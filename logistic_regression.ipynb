{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression from scratch\n",
    "**We will :**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "* [scikit-learn](http://scikit-learn.org/stable/) a library with Simple and efficient tools for data mining and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "we will use the make_classification data from sklearn\n",
    "\n",
    "Loading the data by with the  following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y=datasets.make_classification(n_samples=100000, n_features=100,\n",
    "                                    n_informative=100,n_classes=2, n_redundant=0,\n",
    "                                    random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data-split  ##\n",
    "\n",
    "we will split the data with the following distribution \n",
    "- 99% -training set\n",
    "- 1% -test set\n",
    "\n",
    "we will use the sklearn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we reshape the data into  a numpy-array of shape (1, m). After this, our training (and test) dataset is a numpy-array where each column represents one training example. There should be m_train (respectively m_test) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need to reshape our data to column vectors \n",
    "X_train=X_train.reshape(X_train.shape[0],-1).T\n",
    "X_test=X_test.reshape(X_test.shape[0],-1).T\n",
    "y_train=y_train.reshape(y_train.shape[0],-1).T\n",
    "y_test=y_test.reshape(y_test.shape[0],-1).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "### 3.1 - Helper functions\n",
    "\n",
    "**sigmoid**:  implementing `sigmoid()`.  $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Initializing parameters\n",
    "\n",
    "**parameter initilization:** Implementing parameter initialization we  have to initialize w as a vector of zeros. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_initiliazation(dimension):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    w=np.zeros((dimension,1),dtype=float)\n",
    "    b=0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 -forward propagation\n",
    "\n",
    "**forward propagation:** Implementing forward propagation \n",
    "- We get X\n",
    "- We compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,W,b):\n",
    "    \"\"\"\n",
    "    This function computes the forward propagation computation by getting Z then applying the sigmoid function\n",
    "    \n",
    "    Arguments:\n",
    "        X -> input matrix\n",
    "        W-> Weights vector\n",
    "        b-> bias scalar\n",
    "    Returns:\n",
    "        A->Activations vector\n",
    "    \"\"\"\n",
    "    Z=np.dot(W.T,X)+b\n",
    "    A=sigmoid(Z)\n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 -Compute Cost \n",
    "\n",
    "**Cost function:** Computing the cost  \n",
    "- We calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A,Y,m):\n",
    "    \"\"\"\n",
    "    This function computes the cost of the logistic regression\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        A-> Activations from the forward propagation\n",
    "        Y-> The correct labels \n",
    "        m-> The number of examples in the set\n",
    "    Returns:\n",
    "        cost-> logistic regression cost\n",
    "    \"\"\"\n",
    "    cost=-(1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### 3.4 -Back propagation\n",
    "\n",
    "**Back propagation:** We Compute the gradients using the following formulas. these can be verified using calculus{partial derivatives}  \n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_propagation(A,X,Y,m):\n",
    "    \"\"\"\n",
    "    This function computes the gradients \n",
    "    \"\"\"\n",
    "    dw=1/m*np.dot(X,(A-Y).T)\n",
    "    db=1/m*np.sum(A-Y)\n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 -update parameters\n",
    "\n",
    "**Update parameters:** We update parameters after getting the gradients utilizing the following formulas\n",
    "- W=W- $\\alpha$dw\n",
    "- b=b- $\\alpha$db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(W,b,dw,db,learning_rate):\n",
    "    \"\"\"\n",
    "    This function updates the parameters w and b\n",
    "    \n",
    "    Arguments:\n",
    "        W- weights matrix\n",
    "        \n",
    "        b- bias scalar\n",
    "        dw-gradient scalar\n",
    "        db- gradient bias\n",
    "    \"\"\"\n",
    "    W=W-learning_rate*dw\n",
    "    b=b-learning_rate*db\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to use w and b to predict the labels for a dataset X. \n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(W,b,X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (features, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (features, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    z=np.dot(W.T,X)+b\n",
    "    A=sigmoid(z)\n",
    "    return np.choose(A < 0.5,[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  - Merge all functions into a model ##\n",
    "\n",
    "we  will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X_train,y_train,X_test,y_test,num_iterations,learning_rate=0.1,print_costs=True):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "        X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "        Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "        X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "        Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "        num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "        learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "        print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "         d-> a dictionary containing the varios parameters and results\n",
    "    \"\"\"\n",
    "    W,b=parameter_initiliazation(X_train.shape[0])\n",
    "    m=X_train.shape[1]\n",
    "    costs=[]\n",
    "    for i in range(num_iterations):\n",
    "        A=forward_propagation(X_train,W,b)\n",
    "        cost=compute_cost(A,y_train,m)\n",
    "        costs.append(cost)\n",
    "        dw,db=back_propagation(A,X_train,y_train,m)\n",
    "        W,b=update_parameters(W,b,dw,db,learning_rate)\n",
    "        if i%10==0 and print_costs:\n",
    "            print(\"cost after {} iterations is {}\".format(i,cost))\n",
    "       \n",
    "    Y_prediction_train=predict(W,b,X_train)\n",
    "    Y_prediction_test=predict(W,b,X_test)\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : W, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iterations is 0.6931471805599453\n",
      "cost after 10 iterations is 0.4643811720781759\n",
      "cost after 20 iterations is 0.4197591934353743\n",
      "cost after 30 iterations is 0.4005632032867893\n",
      "cost after 40 iterations is 0.3900921286446589\n",
      "cost after 50 iterations is 0.3836970762049911\n",
      "cost after 60 iterations is 0.3795215404661102\n",
      "cost after 70 iterations is 0.3766711245953468\n",
      "cost after 80 iterations is 0.37466210379608506\n",
      "cost after 90 iterations is 0.3732114377241076\n",
      "train accuracy: 83.55757575757576 %\n",
      "test accuracy: 83.2 %\n"
     ]
    }
   ],
   "source": [
    "d=model(X_train,y_train,X_test,y_test,learning_rate=0.01,num_iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Our models gets upto 83% accuracy in both test and train sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### TensorFlow implementation of the Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    \"\"\"\n",
    "    This function creates the placeholders for the X inputs and labels y ,\n",
    "    we set the second dimension in shape to None since we dont want to have a fixed number of inputs to feed\n",
    "    \n",
    "    Arguments:\n",
    "        n_x -> dimension of the input features X\n",
    "        n_y -> dimension of the labels Y\n",
    "    Returns:\n",
    "        X->placeholder for X\n",
    "        Y-> placeholder for Y   \n",
    "        \n",
    "    \"\"\"\n",
    "    X=tf.placeholder(dtype=tf.float32,shape=(n_x,None),name=\"X\")\n",
    "    Y=tf.placeholder(dtype=tf.float32,shape=(n_y,None),name=\"Y\")\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_variables(dimension):\n",
    "    \"\"\"\n",
    "    This function creates the variables for weights W and bias b\n",
    "    \n",
    "    Argument:\n",
    "        dimension-> dimension of the weights in regards to input X\n",
    "    Returns:\n",
    "        W: variable for weights\n",
    "        b: variable for bias\n",
    "    \"\"\"\n",
    "    W=tf.get_variable(dtype=tf.float32,shape=(dimension,1),name=\"W\",initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "    b=tf.get_variable(dtype=tf.float32,name=\"b\",initializer=tf.constant(0.))\n",
    "    \n",
    "    return W,b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def propagate(X,W,b):\n",
    "    \"\"\"\n",
    "    This function runs a forward pass for the network\n",
    "    \n",
    "    Arguments:\n",
    "        X-> input features X\n",
    "        W-> weights matrix\n",
    "        b->bias\n",
    "    Returns:\n",
    "        Activations A\n",
    "    \"\"\"\n",
    "    \n",
    "    Z=tf.add(tf.matmul(tf.transpose(W),X),b)\n",
    "    A=tf.sigmoid(Z)# We it use when not utilizing tf.nn.sigmoid_cross_entropy_with_logits\n",
    "    return A\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_cost(A,Y,m):\n",
    "    \"\"\"\n",
    "    This function computes the cost of the Logistic regression\n",
    "    \n",
    "    Arguments:\n",
    "        A-> predicted labels from the  activation or logits when tf.nn.s.... is used\n",
    "        Y-> true labels \n",
    "    Returns:\n",
    "        cost-> the logistic regression cost\n",
    "    \n",
    "    \"\"\"\n",
    "    cost=-(1/m)*tf.reduce_sum(Y*tf.log(A)+(1-Y)*tf.log(1-A))\n",
    "    #or\n",
    "    #cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=A,labels=Y))#-> no sigmoid activation in the propagate function\n",
    "    \n",
    "    return cost\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tensorflow_model(X_train,X_test,y_train,y_test,learning_rate=0.1,num_iterations=100,print_costs=True):\n",
    "    \"\"\"\n",
    "    Builds the tensorflow  logistic regression using functions above\n",
    "    \n",
    "    Arguments:\n",
    "        X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "        Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "        X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "        Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "        num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "        learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "        print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "         d-> a dictionary containing the varios parameters and results\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    m=X_train.shape[1]\n",
    "    X,Y=create_placeholders(X_train.shape[0],y_train.shape[0])\n",
    "    W,b=create_variables(X_train.shape[0])\n",
    "    A=propagate(X,W,b)\n",
    "    cost=compute_cost(A,Y,m)\n",
    "    \n",
    "    costs_list=[]\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(num_iterations):\n",
    "            _,costs=sess.run([optimizer,cost],feed_dict={X:X_train,Y:y_train})\n",
    "            costs_list.append(costs)  \n",
    "            if i%10==0 and print_costs:\n",
    "                 print(\"cost after {} iterations is {}\".format(i,costs))\n",
    "                \n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predictions_train=np.choose(sess.run(A,feed_dict={X:X_train}) < 0.5,[1,0])\n",
    "        predictions_test=np.choose(sess.run(A,feed_dict={X:X_test}) < 0.5,[1,0])\n",
    "        train_correct_prediction = tf.equal(predictions_train, y_train)\n",
    "        test_correct_prediction = tf.equal(predictions_test, y_test)\n",
    "\n",
    "\n",
    "            # Calculate accuracy on the test set\n",
    "        train_accuracy = tf.reduce_mean(tf.cast(train_correct_prediction, \"float\"))\n",
    "        test_accuracy = tf.reduce_mean(tf.cast(test_correct_prediction, \"float\"))\n",
    "        print(\"The training accuracy is {} and test accuracy is {}\".format(train_accuracy.eval()*100,test_accuracy.eval()*100))\n",
    "\n",
    "        d={'costs':costs_list,\n",
    "          'train_accuracy':train_accuracy.eval(),\n",
    "           'test_accuracy':test_accuracy.eval(),\n",
    "           \"w\" : W.eval(), \n",
    "            \"b\" : b.eval(),\n",
    "          \"learning_rate\" : learning_rate,\n",
    "          \"num_iterations\": num_iterations\n",
    "           \n",
    "          }\n",
    "        return d\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after 0 iterations is 0.693192720413208\n",
      "The training accuracy is 83.56666564941406 and test accuracy is 82.99999833106995\n"
     ]
    }
   ],
   "source": [
    "d=tensorflow_model(X_train,X_test,y_train,y_test,learning_rate=0.1,num_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
