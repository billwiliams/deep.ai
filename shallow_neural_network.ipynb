{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A shallow(2-Layer) Neural Network from scratch\n",
    "**We will :**\n",
    "- Build  the general architecture of a two layer Neural Network  learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order.\n",
    "<img src=./data/2-Layer_Neural_Network.png><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "* [scikit-learn](http://scikit-learn.org/stable/) a library with Simple and efficient tools for data mining and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "we will use the make_classification data from sklearn\n",
    "\n",
    "Loading the data by with the  following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y=datasets.make_classification(n_samples=1000000, n_features=100,\n",
    "                                    n_informative=100,n_classes=2, n_redundant=0,\n",
    "                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data-split  ##\n",
    "\n",
    "we will split the data with the following distribution \n",
    "- 99% -training set\n",
    "- 1% -test set\n",
    "\n",
    "we will use the sklearn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we reshape the data into  a numpy-array of shape (1, m). After this, our training (and test) dataset is a numpy-array where each column represents one training example. There should be m_train (respectively m_test) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to reshape our data to column vectors \n",
    "X_train=X_train.reshape(X_train.shape[0],-1).T\n",
    "X_test=X_test.reshape(X_test.shape[0],-1).T\n",
    "y_train=y_train.reshape(y_train.shape[0],-1).T\n",
    "y_test=y_test.reshape(y_test.shape[0],-1).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will define the parameters of our Neural Network and initialize them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x,n_h,n_y):\n",
    "    \"\"\"\n",
    "    This functions computes the initialization weights and bias for the hidden layer and output layer\n",
    "    \n",
    "    Arguments:\n",
    "        n_x->number of features in the input X\n",
    "        n_h-> number of units in the hidden layer\n",
    "        n_y-> number of units in the output layer\n",
    "    \n",
    "    Returns:\n",
    "        parameters->weights and biases for each layer\n",
    "    \"\"\"\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1),dtype=float)\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1),dtype=float)\n",
    "    \n",
    "    parameters={\"W1\": W1, \"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    #Lets check that parameters have correct shapes\n",
    "    assert parameters[\"W1\"].shape==(n_h,n_x),\"Error in parameters W1 shape\"\n",
    "    assert parameters[\"W2\"].shape==(n_y,n_h),\"Error in parameters W2 shape\"\n",
    "    assert parameters[\"b1\"].shape==(n_h,1),\"Error in parameters b1 shape\"\n",
    "    assert parameters[\"b2\"].shape==(n_y,1),\"Error in parameters b2 shape\"\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some checks to ensure we initiliazed the parameters correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100) (1, 5) (5, 1) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters=initialize_parameters(X_train.shape[0],5,y_train.shape[0])\n",
    "print(parameters[\"W1\"].shape,parameters[\"W2\"].shape,parameters[\"b1\"].shape,parameters[\"b2\"].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define the activations functions mainly Relu and sigmoid and tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <center>  Relu function </center> \n",
    "#    <center>               $\\max(0,Z)$</center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    This function computes the relu activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighteds inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->relu activations of Z\n",
    "    \"\"\"\n",
    "    A=np.maximum(0,Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  <center>  Tanh function </center> \n",
    "#    <center>               $\\frac{\\mathrm{e}^{z}-\\mathrm{e}^{-z}}{\\mathrm{e}^{z}+\\mathrm{e}^{-z}}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    This function computes the tanh activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighted inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->tanh activations of Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A=(np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <center>  Sigmoid function </center> \n",
    "#    <center>               $\\frac{1}{1-\\mathrm{e}^{-z}}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    This function computes the sigmoid activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighted inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->sigmoid activations of Z\n",
    "    \"\"\"\n",
    "    A= 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation\n",
    "\n",
    "**forward propagation:** Implementing forward propagation \n",
    "\n",
    "** for layer hidden layer **\n",
    "- We get X\n",
    "- We compute $A1 = \\sigma(W1^T X + b1) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "\n",
    "** for output layer **\n",
    "- We get layer 1 activations\n",
    "- we compute $A2=\\sigma(W2^T + b2)=(a^{(0)}, a^{(1)}, ..., a^{(n_h-1)}, a^{(n_h)})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(parameters,X,activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    This function computes the forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "        parameters-> weights and biases for hidden layer\n",
    "        activation-> the activation to use \n",
    "    \"\"\"\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    #compute Z1\n",
    "    Z1=np.dot(W1 ,X )+ b1\n",
    "        \n",
    "    if activation==\"relu\":\n",
    "        A1=relu(Z1)\n",
    "    elif activation==\"sigmoid\":\n",
    "        A1=sigmoid(Z1)\n",
    "    elif activation==\"tanh\":\n",
    "        A1=tanh(Z1)\n",
    "        \n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    #activation for the final layer is a sigmoid since we are trying to estimate the predictions where it's either 1 or 0\n",
    "    A2=sigmoid(Z2)\n",
    "    \n",
    "    #we need to keep A2 and A1 for backpropagation\n",
    "    cache={\"A2\":A2,\"A1\":A1,\"Z1\":Z1,\"Z2\":Z2 }\n",
    "    \n",
    "    return A2,cache\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define the cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_cost(A,Y):\n",
    "    \"\"\"\n",
    "    This function computes the cost of the Neural Network    \n",
    "    \n",
    "    Arguments:\n",
    "        A-> Activations from the forward propagation\n",
    "        Y-> The correct labels \n",
    "    Returns:\n",
    "        cost-> logistic regression cost\n",
    "    \"\"\"\n",
    "    m=Y.shape[1]\n",
    "    cost=-(1/m)*np.sum(Y*np.log(A)+((1-Y)*np.log(1-A)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives of the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_derivative(A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of the relu function Max(0,Z)->drelu=Max(0,1)\n",
    "    drelu->returns 0 for all values below and including o and 1 for all other values\n",
    "    Arguments:\n",
    "        A->Activations\n",
    "    Returns:\n",
    "        Ad->relu derivative of A\n",
    "    \"\"\"\n",
    "    \n",
    "    Ad=np.choose(A>0,[0,1])\n",
    "    return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of the sigmoid function\n",
    "    the derivate evaluates to a(1-a) where a is the sigmoid function\n",
    "    Arguments:\n",
    "        A->Activations\n",
    "    Returns:\n",
    "        Ad->relu derivative of A\n",
    "    \"\"\"\n",
    "    Ad=sigmoid(A)*(1-sigmoid(A))\n",
    "    return Ad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_derivative(A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of the tanh function\n",
    "    the derivate evaluates to (1-a*a) where a is the tanh function\n",
    "    Arguments:\n",
    "        A->Activations\n",
    "    Returns:\n",
    "        Ad->tanh derivative of A\n",
    "    \"\"\"\n",
    "    Ad=(1-tanh(A)*tanh(A))\n",
    "    return Ad\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Backpropagation for the 2-layer neural network. \n",
    "- We compute the derivatives dw1,dw2,db1 and db2\n",
    "- These derivatives will be used to update the parameters W1,W2,b1 and b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagation(cache,Y,X,parameters):\n",
    "    \"\"\"\n",
    "    This function computes the gradients by backpropagation \n",
    "    Arguments:\n",
    "        cache-> stored values of activations\n",
    "        parameters->stored parameters\n",
    "        Y->true labels\n",
    "        X->inputs features\n",
    "    Returns:\n",
    "        grads-> a dictionary containing the gradients of the parameters W1,W2,b1,and b2\n",
    "    \n",
    "    \"\"\"\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    A2=cache[\"A2\"]\n",
    "    A1=cache[\"A1\"]\n",
    "    Z1=cache[\"Z1\"]\n",
    "    Z2=cache[\"Z2\"]\n",
    "    \n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    \n",
    "    dZ2=A2-Y\n",
    "    dW2=1/m* np.dot(dZ2,A1.T)\n",
    "    db2=1/m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1=np.dot(W2.T,dZ2)*relu_derivative(Z1)\n",
    "    dW1=1/m*np.dot(dZ1,X.T)\n",
    "    db1=1/m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads={\"dW1\":dW1,\"dW2\":dW2,\"db1\":db1,\"db2\":db2}\n",
    "    \n",
    "    return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predictions for the network\n",
    "- We compute the predictions by using the updated parameters and running a forward propation step \n",
    "- We then convert all values below 0.5 to 0 and all above 0.5 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_propagate(parameters,X,activation=\"relu\")\n",
    "    predictions = np.choose(A2<0.5,[1,0])\n",
    "   \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 2 layer Neural Network model \n",
    "- We combine all the functions above into a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X,Y,n_h,num_iterations,learning_rate):\n",
    "    \"\"\"\n",
    "    This function combines all the above functions to create the 2 layer Neural Network\n",
    "    \n",
    "    Arguments:\n",
    "        X-> inputs X\n",
    "        Y->true labels\n",
    "        num_iterations-> number of iterations\n",
    "        learning_rate-> the model's learning rate\n",
    "        n_h-> number of hidden units in the hidden layer\n",
    "    Returns:\n",
    "        parameters-> a dictionary with the updated parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    np.seterr(all='raise') # raise any numpy errors\n",
    "    costs=[]# a list of the costs over iterations\n",
    "    parameters=initialize_parameters(X.shape[0],n_h,Y.shape[0])\n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    for i in range(num_iterations):\n",
    "        A2,cache=forward_propagate(parameters,X,activation=\"relu\")\n",
    "        cost=compute_cost(A2,Y)\n",
    "        costs.append(cost)\n",
    "        grads=backpropagation(parameters=parameters,cache=cache,X=X,Y=Y)\n",
    "        parameters[\"W1\"]=parameters[\"W1\"]-learning_rate*grads[\"dW1\"]\n",
    "        parameters[\"W2\"]=parameters[\"W2\"]-learning_rate*grads[\"dW2\"]\n",
    "        parameters[\"b1\"]=parameters[\"b1\"]-learning_rate*grads[\"db1\"]\n",
    "        parameters[\"b2\"]=parameters[\"b2\"]-learning_rate*grads[\"db2\"]\n",
    "        if i%100==0:\n",
    "            preds=predict(parameters,X_train)\n",
    "            print(\"cost on iteration {} is {}\".format(i,cost))\n",
    "            print(\"train accuracy on iteration {} is {} %\".format(i,(100 - np.mean(np.abs(preds - y_train)) * 100)))\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Cost vs number of iterations\")\n",
    "    plt.grid(True)\n",
    "    return parameters\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We now run the model with \n",
    "- 20 hidden units in the hidden layer \n",
    "- 402 iterations \n",
    "- learning rate of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost on iteration 0 is 0.6927748868052204\n",
      "train accuracy on iteration 0 is 55.385252525252525 %\n",
      "cost on iteration 100 is 0.18685378916955542\n",
      "train accuracy on iteration 100 is 93.88323232323232 %\n",
      "cost on iteration 200 is 0.10758448042536403\n",
      "train accuracy on iteration 200 is 97.18070707070707 %\n",
      "cost on iteration 300 is 0.0831305110071814\n",
      "train accuracy on iteration 300 is 98.09646464646465 %\n",
      "cost on iteration 400 is 0.07222192506564194\n",
      "train accuracy on iteration 400 is 98.48838383838384 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKIiISAMVBRERafD/Abb5jezVWhbMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a280000ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params=model(X_train,y_train,20,402,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 98.29 %\n"
     ]
    }
   ],
   "source": [
    "preds=predict(params,X_test)\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(preds - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " ##  Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Initialize placeholders for the inputs X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_placeholders(n_x,n_y):\n",
    "    \"\"\"\n",
    "    This function  creates X and Y placeholders\n",
    "    \n",
    "    Arguments:\n",
    "        n_x-> number of features (dimension)\n",
    "        n_y -> dimension of output (row)\n",
    "    Returns:\n",
    "        X-> placeholder for X\n",
    "        Y-> placeholder for Y\n",
    "    \"\"\"\n",
    "    X= tf.placeholder(shape=(n_x,None),dtype=tf.float32,name=\"X\")\n",
    "    Y=tf.placeholder(shape=(n_y,None),dtype=tf.float32,name=\"Y\")\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize variables W1,W2,b1 and b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_variables(n_x,n_h,n_y):\n",
    "    \"\"\"\n",
    "    This function initializes variables W1,W2,b1 and b2\n",
    "    Arguments:\n",
    "        n_x-> number of features\n",
    "        n_h-> units in hidden layer,\n",
    "        n_y->units in output layer\n",
    "    Returns:\n",
    "        parameters-> a dictionary containing the parameters W1,W2,b1 and b2\n",
    "    \"\"\"\n",
    "    W1=tf.get_variable(shape=(n_h,n_x),dtype=tf.float32,name=\"W1\",initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b1=tf.get_variable(shape=(n_h,1),dtype=tf.float32,name=\"b1\",initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "    \n",
    "    W2=tf.get_variable(shape=(n_y,n_h),dtype=tf.float32,name=\"W2\",initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    b2=tf.get_variable(shape=(n_y,1),dtype=tf.float32,name=\"b2\",initializer=tf.zeros_initializer(dtype=tf.float32))\n",
    "    \n",
    "    parameters={\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2,Y):\n",
    "    \"\"\"\n",
    "    This function computes the cost of the Neural Network    \n",
    "    \n",
    "    Arguments:\n",
    "        A-> Activations from the forward propagation(logits)\n",
    "        Y-> The correct labels \n",
    "    Returns:\n",
    "        cost-> logistic regression cost\n",
    "    \n",
    "    \"\"\"\n",
    "    m=Y.shape[1]\n",
    "    cost =tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=A2,labels=Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now define the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(X,Y,parameters,activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    This function computes the forward propagation for the network\n",
    "    Arguments:\n",
    "        X->input features X\n",
    "        Y-> labels Y\n",
    "        activation-> type of activation\n",
    "    Returns:\n",
    "        Z2-> weighted layer two values\n",
    "    \n",
    "    \"\"\"\n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    \n",
    "    Z1=tf.add(tf.matmul(W1,X), b1)\n",
    "    if activation==\"relu\":\n",
    "        A1=tf.nn.relu(Z1)\n",
    "    elif activation==\"tanh\":\n",
    "        A1=tf.nn.tanh(Z1)\n",
    "    elif activation==\"sigmoid\":\n",
    "        A1=tf.nn.sigmoid(Z1)\n",
    "    Z2=tf.add(tf.matmul(W2,A1),b2)\n",
    "    \n",
    "    return Z2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define the predict function which returns the sigmoid activations of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Z2):\n",
    "    \"\"\"\n",
    "    This function computes the predictions \n",
    "    Arguments:\n",
    "        Z2->weighted outputs for layer 2\n",
    "    Returns:\n",
    "        A2-> activations for output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    A2=tf.nn.sigmoid(Z2)\n",
    "    \n",
    "    return A2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To train on minibatch gradient descent we create a function to generate minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(X,Y,batch_size=64,seed=0):\n",
    "    \"\"\"\n",
    "    This function creates minibatches given X and Y\n",
    "    Arguments:\n",
    "        X-> features\n",
    "        Y-> labels\n",
    "        batch_size-> size of batch to use\n",
    "        seed-> numpy seed to generate \n",
    "    Returns:\n",
    "        minibatches-> a list of minibatches generated\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m=X_train.shape[1]\n",
    "    number_of_batches=m//batch_size\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X=X[:,permutation]\n",
    "    shuffled_Y=Y[:,permutation].reshape(1,m)\n",
    "    minibatches=[]\n",
    "    for k in range(number_of_batches):\n",
    "        mini_batch_X=shuffled_X[:,k*batch_size:(k+1)*batch_size]\n",
    "        mini_batch_y=shuffled_Y[:,k*batch_size:(k+1)*batch_size]\n",
    "        minibatch=(mini_batch_X,mini_batch_y)\n",
    "        minibatches.append(minibatch)\n",
    "    if m%batch_size!=0:\n",
    "        last_batch_X=shuffled_X[:,number_of_batches*batch_size:m]\n",
    "        last_batch_y=shuffled_Y[:,number_of_batches*batch_size:m]\n",
    "        last_mini_batch=(last_batch_X,last_batch_y)\n",
    "        minibatches.append(last_mini_batch)\n",
    "    return minibatches\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define the model function to create the tensorflow 2 layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train,y_train,X_test,y_test,num_iterations,hidden_units,learning_rate):\n",
    "    \"\"\"\n",
    "    This function creates the tensorflow 2 layer neural network\n",
    "    \n",
    "    Arguments:\n",
    "        X_train-> training inputs\n",
    "        y_train-> training labels\n",
    "        num_iterations-> number of iterations to compute gradient descent\n",
    "        hidden_units->number of hidden units in the hidden layer\n",
    "        learning_rate-> the learning rate for the gradient descent\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph() #reset the graph\n",
    "    logs_path = \"./tb2/\" #tensorboard log directory\n",
    "    \n",
    "    # initialize the placeholders\n",
    "    X,Y=initialize_placeholders(X_train.shape[0],y_train.shape[0])\n",
    "    # initialize parameters\n",
    "    parameters=initialize_variables(X_train.shape[0],hidden_units,y_train.shape[0])\n",
    "    # compute forward propagation i.e the last activation is avoided since the cost function computes the sigmoid\n",
    "    Z2=propagate(X,Y,parameters,\"relu\")\n",
    "    #compute the loss function\n",
    "    cost= compute_cost(Z2,Y)\n",
    "    # define the optimizer\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    #define the predictions\n",
    "    preds=predict(Z2)\n",
    "    # define the Loss for ploting in the tensorboard view\n",
    "    cost_summary= tf.summary.scalar(\"Loss\", cost)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    init=tf.global_variables_initializer()\n",
    "    # create session \n",
    "    with tf.Session() as sess:\n",
    "        #initialize variables\n",
    "        sess.run(init)\n",
    "        # Tensorboard writer \n",
    "        writer = tf.summary.FileWriter(logs_path,graph=tf.get_default_graph())\n",
    "        \n",
    "        #seed for generating minibatches\n",
    "        seed=10\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            seed=seed+1\n",
    "            # evaluate cost and cost_summary \n",
    "            minibatches=minibatch(X_train,y_train,batch_size=64,seed=seed)\n",
    "            for mini_batch in minibatches:\n",
    "                (minibatch_X,minibatch_Y)= mini_batch\n",
    "                summary,_,costs=sess.run([cost_summary,optimizer,cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            \n",
    "                writer.add_summary(summary,i)\n",
    "            if i%10==0:\n",
    "                #evaluate train  and test predictions\n",
    "                train_preds=np.choose(preds.eval(feed_dict={X:X_train})<0.5,[1,0])\n",
    "                train_correct_prediction = tf.equal(train_preds, y_train)\n",
    "                train_accuracy = tf.reduce_mean(tf.cast(train_correct_prediction, \"float\"))\n",
    "                test_preds=np.choose(preds.eval(feed_dict={X:X_test})<0.5,[1,0])\n",
    "                test_correct_prediction = tf.equal(test_preds, y_test)\n",
    "                test_accuracy = tf.reduce_mean(tf.cast(test_correct_prediction, \"float\"))\n",
    "                print(\"Cost:{}\".format(costs))\n",
    "                print(\"Training accuracy after {} epochs is {}\".format(i,train_accuracy.eval()))\n",
    "                print(\"Test accuracy after {} epochs is {}\".format(i,test_accuracy.eval()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now create the model \n",
    "- Our computation graph is as follows \n",
    "- We use 11 epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data/graph.png\"><img>"
   ]
  },
  {
   "cell_type": "code",
  "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:0.007514956872910261\n",
      "Training accuracy after 0 epochs is 0.9947586059570312\n",
      "Test accuracy after 0 epochs is 0.9948999881744385\n",
      "Cost:0.007274497766047716\n",
      "Training accuracy after 10 epochs is 0.9947707056999207\n",
      "Test accuracy after 10 epochs is 0.9950000047683716\n"
     ]
    }
   ],
   "source": [
    "model(X_train=X_train,y_train=y_train,X_test=X_test,y_test=y_test,num_iterations=11,hidden_units=100,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
