{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A shallow(2-Layer) Neural Network from scratch\n",
    "**We will :**\n",
    "- Build  the general architecture of a two layer Neural Network  learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order.\n",
    "<img src=./data/2-Layer_Neural_Network.png><img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "* [scikit-learn](http://scikit-learn.org/stable/) a library with Simple and efficient tools for data mining and data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dataset ##\n",
    "\n",
    "we will use the make_classification data from sklearn\n",
    "\n",
    "Loading the data by with the  following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y=datasets.make_classification(n_samples=100000, n_features=100,\n",
    "                                    n_informative=100,n_classes=2, n_redundant=0,\n",
    "                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Data-split  ##\n",
    "\n",
    "we will split the data with the following distribution \n",
    "- 99% -training set\n",
    "- 1% -test set\n",
    "\n",
    "we will use the sklearn train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we reshape the data into  a numpy-array of shape (1, m). After this, our training (and test) dataset is a numpy-array where each column represents one training example. There should be m_train (respectively m_test) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to reshape our data to column vectors \n",
    "X_train=X_train.reshape(X_train.shape[0],-1).T\n",
    "X_test=X_test.reshape(X_test.shape[0],-1).T\n",
    "y_train=y_train.reshape(y_train.shape[0],-1).T\n",
    "y_test=y_test.reshape(y_test.shape[0],-1).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will define the parameters of our Neural Network and initialize them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x,n_h,n_y):\n",
    "    \"\"\"\n",
    "    This functions computes the initialization weights and bias for the hidden layer and output layer\n",
    "    \n",
    "    Arguments:\n",
    "        n_x->number of features in the input X\n",
    "        n_h-> number of units in the hidden layer\n",
    "        n_y-> number of units in the output layer\n",
    "    \n",
    "    Returns:\n",
    "        parameters->weights and biases for each layer\n",
    "    \"\"\"\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1),dtype=float)\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1),dtype=float)\n",
    "    \n",
    "    parameters={\"W1\": W1, \"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    #Lets check that parameters have correct shapes\n",
    "    assert parameters[\"W1\"].shape==(n_h,n_x),\"Error in parameters W1 shape\"\n",
    "    assert parameters[\"W2\"].shape==(n_y,n_h),\"Error in parameters W2 shape\"\n",
    "    assert parameters[\"b1\"].shape==(n_h,1),\"Error in parameters b1 shape\"\n",
    "    assert parameters[\"b2\"].shape==(n_y,1),\"Error in parameters b2 shape\"\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do some checks to ensure we initiliazed the parameters correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100) (1, 5) (5, 1) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters=initialize_parameters(X_train.shape[0],5,y_train.shape[0])\n",
    "print(parameters[\"W1\"].shape,parameters[\"W2\"].shape,parameters[\"b1\"].shape,parameters[\"b2\"].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define the activations functions mainly Relu and sigmoid and tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <center>  Relu function </center> \n",
    "#    <center>               $\\max(0,Z)$</center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    This function computes the relu activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighteds inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->relu activations of Z\n",
    "    \"\"\"\n",
    "    A=np.maximum(0,Z)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  <center>  Tanh function </center> \n",
    "#    <center>               $\\frac{\\mathrm{e}^{z}-\\mathrm{e}^{-z}}{\\mathrm{e}^{z}+\\mathrm{e}^{-z}}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    This function computes the tanh activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighted inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->tanh activations of Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A=(np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <center>  Sigmoid function </center> \n",
    "#    <center>               $\\frac{1}{1-\\mathrm{e}^{-z}}$</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    This function computes the sigmoid activation \n",
    "    \n",
    "    Arguments:\n",
    "        Z-> Weighted inputs (Z=W.TA+b)\n",
    "    Returns:\n",
    "        A->sigmoid activations of Z\n",
    "    \"\"\"\n",
    "    A= 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation\n",
    "\n",
    "**forward propagation:** Implementing forward propagation \n",
    "\n",
    "** for layer hidden layer **\n",
    "- We get X\n",
    "- We compute $A1 = \\sigma(W1^T X + b1) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "\n",
    "** for output layer **\n",
    "- We get layer 1 activations\n",
    "- we compute $A2=\\sigma(W2^T + b2)=(a^{(0)}, a^{(1)}, ..., a^{(n_h-1)}, a^{(n_h)})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(parameters,X,activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    This function computes the forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "        parameters-> weights and biases for hidden layer\n",
    "        activation-> the activation to use \n",
    "    \"\"\"\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    #compute Z1\n",
    "    Z1=np.dot(W1 ,X )+ b1\n",
    "        \n",
    "    if activation==\"relu\":\n",
    "        A1=relu(Z1)\n",
    "    elif activation==\"sigmoid\":\n",
    "        A1=sigmoid(Z1)\n",
    "    elif activation==\"tanh\":\n",
    "        A1=tanh(Z1)\n",
    "        \n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    #activation for the final layer is a sigmoid since we are trying to estimate the predictions where it's either 1 or 0\n",
    "    A2=sigmoid(Z2)\n",
    "    \n",
    "    #we need to keep A2 and A1 for backpropagation\n",
    "    cache={\"A2\":A2,\"A1\":A1 }\n",
    "    \n",
    "    return A2,cache\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define the cost function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_cost(A,Y):\n",
    "    \"\"\"\n",
    "    This function computes the cost of the Neural Network    \n",
    "    \n",
    "    Arguments:\n",
    "        A-> Activations from the forward propagation\n",
    "        Y-> The correct labels \n",
    "    Returns:\n",
    "        cost-> logistic regression cost\n",
    "    \"\"\"\n",
    "    m=Y.shape[1]\n",
    "    cost=-(1/m)*np.sum(Y*np.log(A)+((1-Y)*np.log(1-A)))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivatives of the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_derivative(A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of the relu function Max(0,Z)->drelu=Max(0,1)\n",
    "    drelu->returns 0 for all values below and including o and 1 for all other values\n",
    "    Arguments:\n",
    "        A->Activations\n",
    "    Returns:\n",
    "        Ad->relu derivative of A\n",
    "    \"\"\"\n",
    "    \n",
    "    Ad=np.choose(A>0,[0,1])\n",
    "    return Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of the sigmoid function\n",
    "    the derivate evaluates to a(1-a) where a is the sigmoid function\n",
    "    Arguments:\n",
    "        A->Activations\n",
    "    Returns:\n",
    "        Ad->relu derivative of A\n",
    "    \"\"\"\n",
    "    Ad=sigmoid(A)*(1-sigmoid(A))\n",
    "    return Ad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_derivative(A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of the tanh function\n",
    "    the derivate evaluates to (1-a*a) where a is the tanh function\n",
    "    Arguments:\n",
    "        A->Activations\n",
    "    Returns:\n",
    "        Ad->tanh derivative of A\n",
    "    \"\"\"\n",
    "    Ad=(1-tanh(A)*tanh(A))\n",
    "    return Ad\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagation(cache,Y,X,parameters):\n",
    "    \"\"\"\n",
    "    This function computes the gradients by backpropagation \n",
    "    Arguments:\n",
    "        cache-> stored values of activations\n",
    "        parameters->stored parameters\n",
    "        Y->true labels\n",
    "        X->inputs features\n",
    "    Returns:\n",
    "        grads-> a dictionary containing the gradients of the parameters W1,W2,b1,and b2\n",
    "    \n",
    "    \"\"\"\n",
    "    m=X.shape[1]\n",
    "    \n",
    "    A2=cache[\"A2\"]\n",
    "    A1=cache[\"A1\"]\n",
    "    \n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    \n",
    "    dZ2=A2-Y\n",
    "    dW2=1/m* np.dot(dZ2,A1.T)\n",
    "    db2=1/m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1=np.dot(W2.T,dZ2)*relu_derivative(A1)\n",
    "    dW1=1/m*np.dot(dZ1,X.T)\n",
    "    db1=1/m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads={\"dW1\":dW1,\"dW2\":dW2,\"db1\":db1,\"db2\":db2}\n",
    "    \n",
    "    return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A2, cache = forward_propagate(parameters,X,activation=\"relu\")\n",
    "    predictions = np.choose(A2<0.5,[1,0])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(X,Y,num_iterations,learning_rate):\n",
    "    \"\"\"\n",
    "    This function combines all the above functions to create the 2 layer Neural Network\n",
    "    \n",
    "    Arguments:\n",
    "        X-> inputs X\n",
    "        Y->tr\n",
    "    \n",
    "    \"\"\"\n",
    "    np.seterr(all='raise')\n",
    "    parameters=initialize_parameters(X.shape[0],40,Y.shape[0])\n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    for i in range(num_iterations):\n",
    "        A2,cache=forward_propagate(parameters,X,activation=\"relu\")\n",
    "        cost=compute_cost(A2,Y)\n",
    "        #print(cost)\n",
    "        grads=backpropagation(parameters=parameters,cache=cache,X=X,Y=Y)\n",
    "        parameters[\"W1\"]=parameters[\"W1\"]-learning_rate*grads[\"dW1\"]\n",
    "        parameters[\"W2\"]=parameters[\"W2\"]-learning_rate*grads[\"dW2\"]\n",
    "        parameters[\"b1\"]=parameters[\"b1\"]-learning_rate*grads[\"db1\"]\n",
    "        parameters[\"b2\"]=parameters[\"b2\"]-learning_rate*grads[\"db2\"]\n",
    "        if i%100==0:\n",
    "            preds=predict(parameters,X_train)\n",
    "            print(cost)\n",
    "            print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(preds - y_train)) * 100))\n",
    "    return parameters\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693478094193\n",
      "train accuracy: 51.16868686868687 %\n",
      "0.666921585739\n",
      "train accuracy: 75.68080808080808 %\n",
      "0.511462647306\n",
      "train accuracy: 80.15959595959596 %\n",
      "0.370568309501\n",
      "train accuracy: 81.78080808080809 %\n",
      "0.306539343937\n",
      "train accuracy: 82.58282828282829 %\n"
     ]
    }
   ],
   "source": [
    "params=model(X_train,y_train,500,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds=predict(params,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 91.4 %\n"
     ]
    }
   ],
   "source": [
    " print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(preds - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-538.80491176060673"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
